%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{amsmath}
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Hidden Echoes Survive Training in Audio To Audio Generative Instrument Models}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Christopher J. Tralie{\rm 1},\\
    Matt Amery,\\
    Benjamin Douglas,\\
    Ian Utz
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Ursinus College Mathematics And Computer Science\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
As generative techniques pervade the audio domain, there has been increasing interest in identifying whether data that was used to train a model was licensed properly. In this paper, we show that if imperceptible echoes are hidden in the training data, a wide variety of audio to audio architectures (differentiable digital signal processing (DDSP), Realtime Audio Variational autoEncoder (RAVE), and ``Dance Diffusion'') will reproduce these echoes in their outputs. Hiding a single echo is particularly robust, and we show initial results towards hiding longer time spread echo patterns. We also show that echoes make their way into fine tuned models, and they also survive demixing. Hence, this simple, classical idea in watermarking shows a lot of promise for tagging generative audio models. 

\end{abstract}

% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}

\section{Introduction}

This result is surprising!

We use modified ideas from echo hiding \cite{gruhl1996echo}.

There are myriad techniques for generative audio instrument training, but to show the general applicability of this technique in a replicable way, we pick three open architectures with fundamentally different approaches whose code is readily available online: RAVE \cite{caillon2021rave}\footnote{ \url{https://github.com/acids-ircam/RAVE} } Dance Diffusion \cite{evans2022dancediffusion} \footnote{ \url{https://github.com/harmonai-org/sample-generator} }, and differentiable digital signal processing (DDSP) \cite{engelddsp}\footnote{We use our own vanilla implementation of DDSP at \url{ANONYMOUS}}.  Each model is trained on audio only, as opposed to those also involving language models (e.g. \cite{evans2024fast}) or MIDI (e.g. \cite{hawthornemulti}), and each model is trained on a collection of instrument sounds from the same instrument.  Specifically, we train models with different conditions on each of three open datasets to further enhance reproducibility: Groove \cite{groove2019}, VocalSet \cite{wilkins2018vocalset}, and GuitarSet \cite{xi2018guitarset}, which span vocals, and drums, and acoustic guitar, respectively.  We evaluate each model using the respective vocals, drums, and ``other'' stems in the MUSDB18-HQ dataset \cite{musdb18-hq} as inputs to models trained under various conditions.  This is a difficult audio watermarking scenario, as long moments of silence may be present in individual stems.




\section{Methods}

Below we briefly describe the three generative audio to audio models we use, as well as the scheme we use to watermark the training data. Every audio sample in the training sets is converted to a 44100hz mono, as are all of the inputs to the models.

\subsection{Audio To Audio Models}

Explain DDSP, RAVE and Dance Diffusion architectures

DDSP has $\approx$5 million parameters, Rave has $\approx$32 million parameters, and Dance Diffusion $\approx$222 million parameters

We use Dance Diffusion with a 81920 sample size, which means the receptive field spans about 1.86 seconds.  As the authors of \cite{hawthornemulti} note, using a network trained with a smaller context to synthesize audio clips may result in audible timbre shifts from one section to another; however, this is fine for a proof of concept.


\subsection{Echo Hiding}
\label{sec:echohiding}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/RaveCepstra.pdf}
    \caption{An example of cepstra computed on style transfer of a 30 second excerpt of a Prince jazz session at Loring Park.  Rave models trained on data with different echoes at 50, 75, and 100 lead to visible peaks at the respective places in their ceptra on the synthesized clips.}
    \label{fig:ravecepstra}
\end{figure}

Given a discrete audio ``carrier waveform'' $x$, audio watermarking techniques hide a binary payload in a watermarked waveform $\hat{x}$ so that $x$ and $\hat{x}$ are perceptually indistinguishable.  The original echo hiding paper by \cite{gruhl1996echo} accomplishes this by creating two waveforms $x_0$ and $x_1$, each with a single echo; 
\begin{equation}
    \label{eq:echohidesingle}
    \begin{aligned}
        x_0[n] &= x[n] + \alpha x[n - \delta_0] \\
        x_1[n] &= x[n] + \alpha x[n - \delta_1]
    \end{aligned}
\end{equation}

 where $\alpha < 1$ trades off perceptibility and robustness of the watermark, and $\delta_0, \delta_1 \leq $ 100 samples at a 44.1khz sample rate.  These waveforms are then mixed together in windows to create $\hat{x}$ according to the payload; where $x_0$ is fully mixed at the center of a window if the payload contains a 0 at that moment and $x_1$ is fully mixed in if the payload contains a 0.  For a window length of 1024 samples, for instance, this amounts to about 43 bits per second at 44.1khz.  Because the echoes are at such a small shift, temporal aliasing of human hearing makes them less noticeable.
 
 
Since convolution in the time domain is multiplication in the frequency domain, the logarithm of the magnitude of the DFT of a window additively separates the frequency response of the echo from the frequency response of the original signal.  Therefore, the so-called ``cepstrum'' of a windowed signal $x_w$:
 
\begin{equation}
    \label{eq:cepstrum}
    c = \text{ifft} ( \log ( | \text{fft} (x_w) | ) )
\end{equation}
 
yields a signal in which a single echo is a high peak, which is referred to as the ``cepstrum'' $c$ \footnote{\cite{gruhl1996echo} note that it is more mathematically correct take the {\em complex logarithm} of the DFT before taking the inverse DFT, and they further enhance with an autocorrelation.  But we found better results with the traditional cepstrum.}.  Thus, to decode the payload from the watermarked signal, one computes $c$ on each window and infers a 0 if $c[\delta_0] > c[\delta_1]$ or a 1 otherwise.

Since we seek to hide echoes in the training data for generative models, it is unlikely that the models we train will synthesize the windows in the same order they occur in the training set.  Therefore, we do away with the windowing completely and instead hide {\em the same echo} $\delta$ in {\em the entire audio clip} of {\em each waveform in the training data}.  We then examine the cepstrum $c$ of an entire clip that comes out of our models.  To score the cepstrum value at $\delta$ in a loudness-independent way, we compute the {\em z-score} at each lag $i$ as follows.  First, let $\mu_{c}^{a,b}[i]$ be the mean of $c$ on the interval $[a, b]$, excluding $i$:

\begin{equation}
    \mu_{c}^{a,b}[i] = \left( \sum_{\substack{j=a \\ j \neq i}}^{b} c[j] \right) / (b-a)
\end{equation}

and let $\sigma_{c}^{a,b}[i]$ be 

\begin{equation}
    \sigma_{c}^{a,b}[i] = \sqrt{ \left( \sum_{\substack{j=a \\ j \neq i}}^{b} (c[j] - \mu_c[i])^2 \right) / (b-a)}
\end{equation}

then

\begin{equation}
    \label{eq:zscore}
    z[i]_{c}^{a,b} = \mu_{c}^{a,b}[i] / \sigma_{c}^{a,b}[i]
\end{equation}


A model trained on data watermarked with echo $\delta$ works well if $z^{a,b}_{\delta}[\delta] > z^{a,b}_{i}[i], i \neq \delta$.  In our experiments, we use $\alpha = 0.4$, $\delta \in \{50, 76, 76, 100\}$, $a=25$, and $b=125$.  Henceforth, we will assume those parameters and simply refer to these numbers as ``the z-scores $z$.''  Figure~\ref{fig:ravecepstra} shows example cepstra from clips created with different Rave\cite{caillon2021rave} models trained on the GuitarSet \cite{xi2018guitarset} dataset, with various echoes $\delta$.  The peaks and z-scores show that the models reproduce the echoes they were trained on.  We will evaluate this more extensively in the experiment section (Figure~\ref{fig:singleechotable}).

\subsection{Time Spread Echo Patterns}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/RavePNCepstra.pdf}
    \caption{Comparing a 30 second style transfer using a Rave model with a time spread echo pattern $p$ embedded in the training data to one without any pattern.  The cross-correlation of the cepstrum with $p$ peaks for the model with the embedded pattern. }
    \label{fig:ravepncepstra}
\end{figure}

Though we have found single echoes to be robust, the information capacity is low.  Supposing we use echoes between 50 and 100 at integer values, we can store at most ~5.7 bits of information in a single dataset.  To increase the information capacity, we also explore followup work on ``time-spread echo hiding'' \cite{ko2005time} that hides an entire pseudorandom binary sequence $p$ with $L$ bits by scaling, time shifting, and convolving it with the carrier signal $x$:

\begin{equation}
\hat{x} = x * \alpha p_{\delta}, \text{ where } p_{\delta}[n] = 2 p[n - \delta] - 1
\end{equation}

where, to maintain perceptual transparency, $\alpha$ is generally significantly smaller than it is for a single echo; we use $\alpha = 0.01$.  To uncover the hidden pattern, one computes the cepstrum $c$ according to Equation~\ref{eq:cepstrum}, and then does a cross-correlation of $c$ with $(2p - 1)$ to obtain a signal $c^*$.  If the echo pattern is well preserved, then $c^*[\delta] > c^*[i \neq \delta]$.

As in the original echo hiding paper, this work hides $p$ at different offsets $\delta$ in two different signals for hiding a 1 or a 0, but, once again, we hide the same time spread echo at the same lag $\delta=75$ for the entire clip in the training data of our models.  We then compute the $z$-score $z_{c^*}^{a,b}$ on $c^*$ on the model outputs using an equation analogous to Equation~\ref{eq:zscore}, though we set $a = 3, b=L+\delta$, and we also exclude the samples of $c^*$ 3 to the left and 3 to the right when computing $\mu_{c^*}^{a,b}$ and $\sigma_{c^*}^{a,b}$.  Overall, we create 8 different versions of each training set we have, each embedded with a different time spread echo pattern of length $L=1024$.  Furthermore, we ensure that the 28 pairwise Hamming distances between the 8 time spread patterns are approximately uniformly distributed between 0 and 1024.  Figure~\ref{fig:ravepncepstra} shows an example of a style transfer on a model trained on data with the first time spread pattern embedded in all of the training data.


Note that followup work by \cite{xiang2010effective} suggests ensuring that the time spread echo patterns don't have more than two 0's or two 1's in a row, which skews the perturbations in $\hat{x}$ to less perceptible higher frequencies.  In this case, one can also compute an enhanced cross-correlation signal as $c^*[n] - 0.5c^*[n-1] - 0.5c^*[n+1]$.  Though we ensured that our time spread echo patterns satisfied this property, we did not find an improvement in our experiments, so we stick to the original cross-correlation z-score.

\section{Experiments}

To rigorously evaluate the efficacy of our echo watermarks, we train each of our three different model architectures on 3 different datasets:  the training set for Groove \cite{groove2019} ($\approx$8 hours), the entire VocalSet dataset \cite{wilkins2018vocalset} ($\approx$6 hours), and the entire GuitarSet dataset \cite{xi2018guitarset} ($\approx$3 hours).  For each model+architecture combination, we train a variety of models with different embedded echo patterns in the training set.  Once each model is trained, we send through as input multiple random segments of lengths 5, 10, 30, and 60 seconds, drawn from each of the 100 corresponding stems in the MUSDB18-HQ dataset \cite{musdb18-hq}.  In particular, models trained on VocalSet get the ``vocals'' stems, models trained on Groove get the ``drums'' stems, and models trained on Guitarset get ``other'' stems (which are mostly acoustic and electric guitar).  Finally, we report z-scores for various single echo and time spread echo patterns on the outputs of the models.

We train Rave for 1.3 million steps for Groove and 2 million steps for GuitarSet and VocalSet.  We train Dance Diffusion for 50,000 steps on all models, and we train DDSP for 500,000 samples on all models.

\subsection{Single Echo Experiments}
\label{sec:experimentssingleecho}


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/RaveZScoreExamples.pdf}
    \caption{As this example with various tagged VocalSet training data shows, the z-scores for a 75 echo are much higher for the models that are trained on a dataset with a 75 echo embedded in every clip, and the separation increases with increasing clip duration.}
    \label{fig:ravezscoreexamples}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/DDSPZScoreExamples.pdf}
    \caption{DDSP models show the strongest preservation of echoes over all model types, as measured by the z-score.}
    \label{fig:ddspzscoreexamples}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/DanceDiffusionZScoreExamples.pdf}
    \caption{Dance Diffusion models show slightly weaker z-scores that may be mixed up between adjacent echoes, but they still reproduce the correct echoes overall.}
    \label{fig:dancediffusionzscoreexamples}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/AllSingleEchoZScores.pdf}
    
    \caption{The means and standard deviations of z-scores for datasets embedded with various echoes (along each inner row) evaluated for different echoes (along each inner column) show that all architectures (outer rows) only strongly reproduce the echoes that they were trained on across all datasets (outer columns).}
    \label{fig:singleechotable}
\end{figure*}

For these experiments, we train each architecture on each of the original VocalSet, GuitarSet, and Groove datasets, as well as on each of these datasets with an embedded echo of 50, 75, 76, and 100.  Figures~\ref{fig:ravezscoreexamples},~\ref{fig:ddspzscoreexamples}, and~\ref{fig:dancediffusionzscoreexamples} show distributions of z-scores for models trained with an echo of 75 and tested with the corresponding stems.  Figure~\ref{fig:singleechotable} shows the mean and standard deviation of z-scores for the MUSDB18-HQ clips over all architectures over all instruments over all echoes.  The echoes are quite robust over all architectures.  The only weakness is a mixup of the adjacent echoes 75 and 76 for the Dance Diffusion models.


TODO: Show percentage tagged




\subsection{Time Spread Echo Sequences}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/DrumsPNModel_pn0.pdf}
    \caption{The further away the perturbed correlated pattern $p'$ gets from the truly embedded pattern $p$, the smaller the z-scores get, increasing the AUROC of z-scores from $p$ and $p'$.  Increasing the length of the synthesized clip (depicted as color) also leads to stronger detection capability.  Finally, the z-scores of $p$ in the embedded model are easily distinguishable from the z-scores of $p$ in the clean model (x's in left plot, dotted lines in right plot).}
    \label{fig:drumspn7}
\end{figure}

For these experiments, we train Rave and DDSP on 8 time spread echo patterns embedded in each dataset.  We omit fully training dance diffusion with these patterns due to computational constraints, and due to the fact that fine tuning with these patterns shows poor results.

To evaluate the results, we once again examine the outputs of multiple random clips from the 100 examples in the MUSDB18-HQ training set.  To get a clearer picture of the extent to which each model captures the time spread pattern, we compute z-scores on the $c^*$ correlating with the original pattern $p$ on the output cepstra, and we also compute z-scores after correlating with a perturbed version $p'$ of $p$ with an increasing number of bits randomly flipped.  To quantify how the z-scores change, we compute an ROC curve, where the true positives are z-scores correlating to $p$, and the false positives are correlating to the perturbed versions $p'$.  

Figure~\ref{fig:drumspn7} shows an example of this evaluation on the first time spread echo pattern embedded in the Groove dataset.  The right plot shows the corresponding ROC curves for 512 bits flipped at different durations, as well as ROC curves where the false positives are z-scores in a clean model correlating $p$.



Inter-model comparisons of z-scores are more challenging here compared to single echoes due to the variation in embedding strength from model to model.   However, within each model we always get a positive slope in AUROC vs bits flipped, and we can always tell the difference with the clean model, both which indicate that the correct echo patterns survive training.


\section{Additional Use Cases}

We now report preliminary results on few additional uses cases to highlight the promise of the hidden echoes for tagging generative audio models.



\subsection{Dance Diffusion Fine Tuning}

Show results only on groove and vocalset since guitarset is too small


\subsection{Single Echo Demixing}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/DemucsZScores.pdf}
    \caption{If we first mix together outputs trained on Groove with an echo of 50, GuitarSet with an echo of 75, and VocalSet with an echo of 100, the correct echoes pop out in the demixed tracks.}
    \label{fig:demucszscores}
\end{figure*}

In realistic applications of audio to audio style transfer, it is common to treat the result as a stem and {\em mix} it in with other tracks.  Hence, we perform a cursory experiment to see the extent to which the synthesized echoes survive mixing and demixing.  We use the ``hybrid Demucs'' algorithm \cite{defossez2019music} to demix the audio.  This demixing model was trained on (among other data) the MUSDB18-HQ training set, so we switch the inputs to the 50 clips from the MUSDB18-HQ {\em test set}.  

To create our testing data, for each architecture, we input the drums stem to the model trained on Groove with a 50 sample echo, the ``other'' stem to the model trained on the GuitarSet data with a 75 sample echo, and the vocals stem to the model trained on VocalSet with a 100 sample echo.  We then mix the results together with equal weights and demix them with Demucs into the drums, vocals, and ``other'' track.  Finally, we compute z-scores on each demixed track at echoes of 50, 75, 76, and 100.  Figure~\ref{fig:demucszscores} shows the results.  The trends are similar to the overall single echo z-scores in in Figure~\ref{fig:singleechotable}, albeit with slightly weaker z-scores.  But, amazingly, all of the correct echoes pop out in their corresponding tracks.


\subsection{Rave Pitch Shift Augmentation}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/RavePercPitchShift.pdf}
    \caption{Z-scores decrease for an increasing probability of pitch augmentation, though even when pitch shifting 100\% of the time, the z-scores are still distinguishable from those of the clean model.}
    \label{fig:pitchshiftaugmentation}
\end{figure}

Data augmentation is an important part of training deep generative models.  One form of data augmentation commonly used in audio is pitch shifting.  Unfortunately, classical watermarks are known to be quite vulnerable to pitch shifting attacks \cite{hu2014variable}.  Echo hiding is no exception; a shift in pitch up by a factor of $f$ will shift the echo down by a factor of $f$; therefore, we would expect degraded results in the presence of pitch shifting augmentation.  To quantify this, we design an experiment training Rave on the Guitarset data embedded with a single echo at 75 samples, for varying degrees of pitch augmentation, and we test on the MUSDB18-HQ dataset as before.  Pitch shifting is disabled by default in Rave, but when it is enabled, it randomly pitch shifts a clip 50\% of the time with simple spline interpolation at the sample level.  Hence, we modify the Rave code to use higher quality pitch shifting with the Rubberband Library \cite{cannam2024pyrubberband}, and we enable a variable probability for pitch shifting.  When pitch shifting happens for a clip in a batch, we pick a factor uniformly at random in the interval $[0.75, 1.25]$.  Figure~\ref{fig:pitchshiftaugmentation} shows the results for training Rave with an increasing probability of pitch shift augmentation under these conditions.  As expected, the results degrade with increasing amounts of pitch shifting, though for the default value of 50\% pitch shifting, the z-scores are still quite far from the clean distribution.  Surprisingly, even at 100\% pitch shifting, the z-scores are still somewhat distinguishable from the clean model, as seen by the AUROC score using the clean model as the false positive distribution.

\subsection{Tagging Datasets}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/Rave_Tagging_MaleFemale.pdf}
    \caption{Tagging male vs female in VocalSet, training with Rave}
    \label{fig:pitchshiftaugmentation}
\end{figure}




\section{Discussion}
Talk about future work fine tuning audio language models like stable audio


\bibliography{writeup}

\end{document}
