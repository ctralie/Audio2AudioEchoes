{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60dc83a-6480-4381-8927-54a0745063be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import pyrubberband as pyrb\n",
    "import sys\n",
    "import gc\n",
    "from scipy.signal import fftconvolve, correlate\n",
    "from scipy.io import wavfile\n",
    "\n",
    "hann = lambda win: 0.5*(1-np.cos(2*np.pi*np.arange(win)/win))\n",
    "\n",
    "def get_cepstrum(x):\n",
    "    \"\"\"\n",
    "    Compute the cepstrum of an entire chunk of audio\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray(N)\n",
    "        Audio samples\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray(N)\n",
    "        Cepstrum\n",
    "    \"\"\"\n",
    "    x = x*hann(x.size)\n",
    "    F = np.abs(np.fft.rfft(x))\n",
    "    F = np.fft.irfft(np.log(F+1e-8))\n",
    "    return F\n",
    "\n",
    "\n",
    "def get_z_score(c, delta, buff=0, start_buff=0):\n",
    "    \"\"\"\n",
    "    Compute a z-score for the a correlation vector or cepstrum\n",
    "    at a particular offset\n",
    "    The mean/std are computed ignoring the offset location, and\n",
    "    there is the option to ignore locations from the beginning or\n",
    "    slightly to the left / slightly to the right of the location\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    c: ndarray(N)\n",
    "        Correlation vector/cepstrum\n",
    "    delta: int\n",
    "        Delay at which to check for the pseudorandom sequence\n",
    "    buff: int\n",
    "        Buffer on either side of delta to ignore when computing mu/std\n",
    "        for z-score\n",
    "    start_buff: int\n",
    "        Ignore this many from the start when computing mu/std \n",
    "        for z-score\n",
    "    \"\"\"\n",
    "    cmu = np.array(c)\n",
    "    if start_buff > 0:\n",
    "        cmu[0:start_buff] = np.nan\n",
    "    cmu[delta-buff:delta+buff+1] = np.nan\n",
    "    mu = np.nanmean(cmu)\n",
    "    std = np.nanstd(cmu)\n",
    "    return (c[delta]-mu)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4c432-995a-450c-ba8a-d25d74402af3",
   "metadata": {},
   "source": [
    "## Single Echo Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f9df6-8951-4f23-816f-de67909e2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, sr = librosa.load(\"../Writeup/supplementary/prince.mp3\", sr=44100)\n",
    "\n",
    "lag_start = 25\n",
    "lag_end = 150\n",
    "rg = np.arange(lag_start, lag_end+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf35e8-dacd-4edf-802b-77afb1f7231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for instrument in [\"groove\", \"guitarset\", \"vocalset\"]:\n",
    "    model = torch.jit.load(f\"../ArtistProtectModels/SingleEchoes/Rave/{instrument}_clean.ts\").eval()\n",
    "    z = model.encode(torch.from_numpy(x).reshape(1,1,-1))\n",
    "    yclean = model.decode(z).numpy().reshape(-1)\n",
    "    wavfile.write(f\"../Writeup/supplementary/rave_prince_{instrument}_clean.wav\", sr, yclean)\n",
    "    \n",
    "    model = torch.jit.load(f\"../ArtistProtectModels/SingleEchoes/Rave/{instrument}_50.ts\").eval()\n",
    "    z = model.encode(torch.from_numpy(x).reshape(1,1,-1))\n",
    "    y50 = model.decode(z).numpy().reshape(-1)\n",
    "    wavfile.write(f\"../Writeup/supplementary/rave_prince_{instrument}_50.wav\", sr, yclean)\n",
    "    \n",
    "    model = torch.jit.load(f\"../ArtistProtectModels/SingleEchoes/Rave/{instrument}_75.ts\").eval()\n",
    "    z = model.encode(torch.from_numpy(x).reshape(1,1,-1))\n",
    "    y75 = model.decode(z).numpy().reshape(-1)\n",
    "    wavfile.write(f\"../Writeup/supplementary/rave_prince_{instrument}_75.wav\", sr, yclean)\n",
    "    \n",
    "    model = torch.jit.load(f\"../ArtistProtectModels/SingleEchoes/Rave/{instrument}_100.ts\").eval()\n",
    "    z = model.encode(torch.from_numpy(x).reshape(1,1,-1))\n",
    "    y100 = model.decode(z).numpy().reshape(-1)\n",
    "    wavfile.write(f\"../Writeup/supplementary/rave_prince_{instrument}_100.wav\", sr, yclean)\n",
    "    \n",
    "    \n",
    "    fac = 0.8\n",
    "    plt.figure(figsize=(fac*10, fac*5))\n",
    "    \n",
    "    cep = get_cepstrum(yclean)\n",
    "    cep = cep[rg[0]:rg[-1]+1]\n",
    "    plt.plot(rg, cep)\n",
    "    \n",
    "    cep = get_cepstrum(y50)\n",
    "    z50 = get_z_score(cep[0:lag_end+1], 50, start_buff=lag_start)\n",
    "    cep = cep[rg[0]:rg[-1]+1]\n",
    "    plt.plot(rg, cep)\n",
    "    \n",
    "    cep = get_cepstrum(y75)\n",
    "    z75 = get_z_score(cep[0:lag_end+1], 75, start_buff=lag_start)\n",
    "    cep = cep[rg[0]:rg[-1]+1]\n",
    "    plt.plot(rg, cep)\n",
    "    \n",
    "    cep = get_cepstrum(y100)\n",
    "    z100 = get_z_score(cep[0:lag_end+1], 100, start_buff=lag_start)\n",
    "    cep = cep[rg[0]:rg[-1]+1]\n",
    "    plt.plot(rg, cep)\n",
    "    \n",
    "    plt.xlabel(\"Echo (Samples)\")\n",
    "    plt.ylabel(\"Cepstrum Value\")\n",
    "    plt.legend([\"Clean\", \"50   (z[50] = {:.1f})\".format(z50), \"75   (z[75] = {:.1f})\".format(z75), \"100 (z[100]={:.1f})\".format(z100)])\n",
    "    name = {\"guitarset\":\"Guitarset\", \"vocalset\":\"VocalSet\", \"groove\":\"Drums\"}[instrument]\n",
    "    plt.title(f\"Prince Jazz Cepstra $c$ for {name} Rave Models\")\n",
    "    \n",
    "    plt.savefig(f\"../Writeup/supplementary/RaveCepstra_{instrument}.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b51231-4340-4c83-b459-8b14048a8f9f",
   "metadata": {},
   "source": [
    "### Dance Diffusion Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bbd27-fca8-454f-ba43-88311247c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(f\"../dance-diffusion/audio_diffusion\")\n",
    "from utils import load_model_for_synthesis, do_style_transfer\n",
    "sample_size = 81920\n",
    "sample_rate = 44100  \n",
    "noise_level = 0.2\n",
    "device = \"cuda\"\n",
    "for instrument in [\"groove\", \"guitarset\", \"vocalset\"]:\n",
    "    model = load_model_for_synthesis(f\"../ArtistProtectModels/SingleEchoes/DanceDiffusion/{instrument}_clean.ckpt\", sample_size, sample_rate, device)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    n = sample_size*(x.size//sample_size)\n",
    "    xi = x[0:n]\n",
    "    xi = torch.from_numpy(xi[None, None, :]).to(device)\n",
    "    y = do_style_transfer(model, xi, steps=100, noise_level=noise_level,device=device)\n",
    "    yclean = y.detach().cpu().numpy()[0, 0, :]\n",
    "    wavfile.write(f\"../Writeup/supplementary/dd_prince_{instrument}_{noise_level}_clean.wav\", sr, yclean)\n",
    "    \n",
    "    model = load_model_for_synthesis(f\"../ArtistProtectModels/SingleEchoes/DanceDiffusion/{instrument}_50.ckpt\", sample_size, sample_rate, device)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    n = sample_size*(x.size//sample_size)\n",
    "    xi = x[0:n]\n",
    "    xi = torch.from_numpy(xi[None, None, :]).to(device)\n",
    "    y = do_style_transfer(model, xi, steps=100, noise_level=noise_level,device=device)\n",
    "    y50 = y.detach().cpu().numpy()[0, 0, :]\n",
    "    wavfile.write(f\"../Writeup/supplementary/dd_prince_{instrument}_{noise_level}_50.wav\", sr, y50)\n",
    "    \n",
    "    model = load_model_for_synthesis(f\"../ArtistProtectModels/SingleEchoes/DanceDiffusion/{instrument}_75.ckpt\", sample_size, sample_rate, device)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    n = sample_size*(x.size//sample_size)\n",
    "    xi = x[0:n]\n",
    "    xi = torch.from_numpy(xi[None, None, :]).to(device)\n",
    "    y = do_style_transfer(model, xi, steps=100, noise_level=noise_level,device=device)\n",
    "    y75 = y.detach().cpu().numpy()[0, 0, :]\n",
    "    wavfile.write(f\"../Writeup/supplementary/dd_prince_{instrument}_{noise_level}_75.wav\", sr, y75)\n",
    "    \n",
    "    model = load_model_for_synthesis(f\"../ArtistProtectModels/SingleEchoes/DanceDiffusion/{instrument}_100.ckpt\", sample_size, sample_rate, device)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    n = sample_size*(x.size//sample_size)\n",
    "    xi = x[0:n]\n",
    "    xi = torch.from_numpy(xi[None, None, :]).to(device)\n",
    "    y = do_style_transfer(model, xi, steps=100, noise_level=noise_level,device=device)\n",
    "    y100 = y.detach().cpu().numpy()[0, 0, :]\n",
    "    wavfile.write(f\"../Writeup/supplementary/dd_prince_{instrument}_{noise_level}_100.wav\", sr, y100)\n",
    "    \n",
    "    \n",
    "    fac = 0.8\n",
    "    plt.figure(figsize=(fac*10, fac*5))\n",
    "    \n",
    "    cep = get_cepstrum(yclean)\n",
    "    cep = cep[rg[0]:rg[-1]+1]\n",
    "    plt.plot(rg, cep)\n",
    "    \n",
    "    cep = get_cepstrum(y50)\n",
    "    z50 = get_z_score(cep[0:lag_end+1], 50, start_buff=lag_start)\n",
    "    cep = cep[rg[0]:rg[-1]+1]\n",
    "    plt.plot(rg, cep)\n",
    "    \n",
    "    cep = get_cepstrum(y75)\n",
    "    z75 = get_z_score(cep[0:lag_end+1], 75, start_buff=lag_start)\n",
    "    cep = cep[rg[0]:rg[-1]+1]\n",
    "    plt.plot(rg, cep)\n",
    "    \n",
    "    cep = get_cepstrum(y100)\n",
    "    z100 = get_z_score(cep[0:lag_end+1], 100, start_buff=lag_start)\n",
    "    cep = cep[rg[0]:rg[-1]+1]\n",
    "    plt.plot(rg, cep)\n",
    "    \n",
    "    plt.xlabel(\"Echo (Samples)\")\n",
    "    plt.ylabel(\"Cepstrum Value\")\n",
    "    plt.legend([\"Clean\", \"50   (z[50] = {:.1f})\".format(z50), \"75   (z[75] = {:.1f})\".format(z75), \"100 (z[100]={:.1f})\".format(z100)])\n",
    "    name = {\"guitarset\":\"Guitarset\", \"vocalset\":\"VocalSet\", \"groove\":\"Drums\"}[instrument]\n",
    "    plt.title(f\"Prince Jazz Cepstra $c$ for {name} Dance Diffusion Models\")\n",
    "    \n",
    "    plt.savefig(f\"../Writeup/supplementary/DDCepstra_{instrument}.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1942e8a-fa1a-4810-995d-78e3ec818e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44d126ca-7e86-49ca-b1d1-1170344e22e7",
   "metadata": {},
   "source": [
    "## Pseudorandom Echo Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda092fe-de25-4439-8488-5a41d8ceca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(f\"../ArtistProtectModels/SingleEchoes/Rave/guitarset_clean.ts\").eval()\n",
    "z = model.encode(torch.from_numpy(x).reshape(1,1,-1))\n",
    "yclean = model.decode(z).numpy().reshape(-1)\n",
    "\n",
    "patt = 0\n",
    "model = torch.jit.load(f\"../ArtistProtectModels/PNEchoes/Rave/guitarset_pn{patt}.ts\").eval()\n",
    "z = model.encode(torch.from_numpy(x).reshape(1,1,-1))\n",
    "ypn = model.decode(z).numpy().reshape(-1)\n",
    "wavfile.write(f\"../Writeup/supplementary/prince_guitarset_pn{patt}.wav\", sr, ypn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360f8d1-b9f2-40c9-baeb-5042dd959cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from prepare_echo_dataset_pn import PN_PATTERNS_1024_8\n",
    "from scipy import signal\n",
    "\n",
    "alpha = 0.01\n",
    "lag = 75\n",
    "q = np.array(PN_PATTERNS_1024_8[patt])\n",
    "L = q.size\n",
    "\n",
    "yh = ypn*hann(ypn.size)\n",
    "s = np.abs(np.fft.rfft(yh))\n",
    "s = np.fft.irfft(np.log(s+1e-8))\n",
    "c = signal.correlate(s, q, mode='valid', method='fft')\n",
    "z = get_z_score(c[0:L+2*lag], lag)\n",
    "\n",
    "yh = yclean*hann(ypn.size)\n",
    "s = np.abs(np.fft.rfft(yh))\n",
    "s = np.fft.irfft(np.log(s+1e-8))\n",
    "c2 = signal.correlate(s, q, mode='valid', method='fft')\n",
    "z2 = get_z_score(c2[0:L+2*lag], lag)\n",
    "\n",
    "rg = np.arange(3, 100)\n",
    "\n",
    "\n",
    "fac = 0.6\n",
    "plt.figure(figsize=(fac*10, fac*5))\n",
    "plt.plot(rg, c2[rg])\n",
    "plt.plot(rg, c[rg])\n",
    "plt.axvline([75], linestyle='--', c='k', linewidth=1)\n",
    "plt.title(\"Prince Jazz Cepstra Cross-Correlation $c^*$ for Guitar Rave Models\")\n",
    "plt.ylabel(\"Cross-Correlation\")\n",
    "plt.xlabel(\"Offset\")\n",
    "plt.legend([\"Clean (z[75]={:.1f})\".format(z2), \"Correct PN Pattern (z[75]={:.1f})\".format(z)])\n",
    "plt.savefig(\"../Writeup/supplementary/RavePNCepstra.svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445612b-fe43-4c24-a297-3df58d0d37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(0.5*(1+PN_PATTERNS_1024_8[patt]), dtype=int)\n",
    "s = \"\".join([str(i) for i in x])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68391839-c6c3-41f5-8fc6-670156f7e278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
